{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614bb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score, \n",
    "                             f1_score, accuracy_score, precision_score, \n",
    "                             recall_score, make_scorer)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import clone\n",
    "from xgboost import XGBClassifier\n",
    "from scipy.stats import loguniform, randint\n",
    "from typing import Dict, Any\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import joblib\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import warnings\n",
    "from joblib import Memory\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "memory = Memory(location='./.cache', verbose=0)\n",
    "\n",
    "\n",
    "class CreditRiskModel:\n",
    "    def __init__(self, model_type: str, metric: str = 'roc_auc'):\n",
    "        self.model_type = model_type\n",
    "        self.metric = metric\n",
    "        self.best_model = None\n",
    "        self.metrics_history = []\n",
    "        self.scorer = self._get_scorer()\n",
    "        self._setup_model_config()\n",
    "\n",
    "    def _get_scorer(self):\n",
    "        scoring = {\n",
    "            'roc_auc': make_scorer(roc_auc_score, needs_proba=True),\n",
    "            'precision': make_scorer(precision_score),\n",
    "            'recall': make_scorer(recall_score),\n",
    "            'f1': make_scorer(f1_score),\n",
    "            'accuracy': make_scorer(accuracy_score),\n",
    "            'average_precision': make_scorer(average_precision_score, needs_proba=True)\n",
    "        }\n",
    "        return scoring.get(self.metric, scoring['roc_auc'])\n",
    "\n",
    "    def _setup_model_config(self):\n",
    "        self.model_config = {\n",
    "            'logistic': {\n",
    "                'model': LogisticRegression(max_iter=1000, class_weight='balanced', solver='saga'),\n",
    "                'params': {\n",
    "                    'classifier__C': loguniform(1e-3, 1e2),\n",
    "                    'classifier__penalty': ['l1', 'l2']\n",
    "                }\n",
    "            },\n",
    "            'random_forest': {\n",
    "                'model': RandomForestClassifier(n_jobs=-1, class_weight='balanced_subsample'),\n",
    "                'params': {\n",
    "                    'classifier__n_estimators': randint(100, 300),\n",
    "                    'classifier__max_depth': randint(5, 30),\n",
    "                    'classifier__min_samples_split': randint(2, 15)\n",
    "                }\n",
    "            },\n",
    "            'xgboost': {\n",
    "                'model': XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, n_jobs=-1, tree_method='hist', scale_pos_weight=1),\n",
    "                'params': {\n",
    "                    'classifier__n_estimators': randint(100, 500),\n",
    "                    'classifier__learning_rate': loguniform(0.01, 0.2),\n",
    "                    'classifier__max_depth': randint(3, 10),\n",
    "                    'classifier__subsample': [0.6, 0.8, 1.0],\n",
    "                    'classifier__colsample_bytree': [0.6, 0.8, 1.0]\n",
    "                }\n",
    "            },\n",
    "            'lightgbm': {\n",
    "                'model': LGBMClassifier(objective='binary', class_weight='balanced', n_jobs=-1),\n",
    "                'params': {\n",
    "                    'classifier__n_estimators': randint(100, 500),\n",
    "                    'classifier__learning_rate': loguniform(1e-3, 0.2),\n",
    "                    'classifier__max_depth': randint(3, 12),\n",
    "                    'classifier__num_leaves': randint(10, 100),\n",
    "                    'classifier__subsample': [0.6, 0.8, 1.0]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _feature_engineering(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df['PAYMENT_RATIO'] = df['PAY_AMT1'] / (df['BILL_AMT1'].abs() + 1)\n",
    "        df['UTILIZATION'] = df['BILL_AMT1'] / (df['LIMIT_BAL'] + 1)\n",
    "        df['AVG_BILL'] = df[[f'BILL_AMT{i}' for i in range(1, 7)]].mean(axis=1)\n",
    "        df['AVG_PAY'] = df[[f'PAY_AMT{i}' for i in range(1, 7)]].mean(axis=1)\n",
    "        return df\n",
    "\n",
    "    def load_data(self, url: str) -> pd.DataFrame:\n",
    "        df = pd.read_excel(url, header=1)\n",
    "        df = df.rename(columns={'default payment next month': 'DEFAULT'})\n",
    "        df = self._feature_engineering(df)\n",
    "        return df\n",
    "\n",
    "    def prepare_data(self, df: pd.DataFrame):\n",
    "        numeric = df.select_dtypes(include=['number']).columns.drop(['ID', 'DEFAULT'], errors='ignore')\n",
    "        X = df[numeric]\n",
    "        y = df['DEFAULT'].astype('int8')\n",
    "        return train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    def build_pipeline(self) -> Pipeline:\n",
    "        return Pipeline([\n",
    "            ('scaler', PowerTransformer(method='yeo-johnson')),\n",
    "            ('classifier', clone(self.model_config[self.model_type]['model']))\n",
    "        ], memory=memory)\n",
    "\n",
    "    def optimize_model(self, X_train, y_train):\n",
    "        pipeline = self.build_pipeline()\n",
    "        search = RandomizedSearchCV(\n",
    "            pipeline,\n",
    "            self.model_config[self.model_type]['params'],\n",
    "            n_iter=30,\n",
    "            cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "            scoring=self.scorer,\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            verbose=0\n",
    "        )\n",
    "        search.fit(X_train, y_train)\n",
    "        return search.best_estimator_, search.best_params_\n",
    "\n",
    "    def evaluate_model(self, model, X_test, y_test) -> Dict[str, float | str]:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        return {\n",
    "            'roc_auc': float(roc_auc_score(y_test, y_proba)),\n",
    "            'average_precision': float(average_precision_score(y_test, y_proba)),\n",
    "            'accuracy': float(accuracy_score(y_test, y_pred)),\n",
    "            'precision': float(precision_score(y_test, y_pred)),\n",
    "            'recall': float(recall_score(y_test, y_pred)),\n",
    "            'f1': float(f1_score(y_test, y_pred)),\n",
    "            'optimization_metric': self.metric\n",
    "        }\n",
    "\n",
    "    def train_and_evaluate(self, url: str) -> Dict[str, Any]:\n",
    "        df = self.load_data(url)\n",
    "        X_train, X_test, y_train, y_test = self.prepare_data(df)\n",
    "\n",
    "        start = time.time()\n",
    "        best_model, best_params = self.optimize_model(X_train, y_train)\n",
    "        train_time = time.time() - start\n",
    "\n",
    "        metrics = self.evaluate_model(best_model, X_test, y_test)\n",
    "        results = {\n",
    "            'model_type': self.model_type,\n",
    "            'best_params': best_params,\n",
    "            'metrics': metrics,\n",
    "            'training_time': train_time,\n",
    "            'features_used': X_train.columns.tolist()\n",
    "        }\n",
    "\n",
    "        self.best_model = best_model\n",
    "        self.metrics_history.append(results)\n",
    "        return results\n",
    "\n",
    "    def save_results(self, results: Dict[str, Any], path: str = './models'):\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        joblib.dump(self.best_model, f'{path}/{self.model_type}_model.pkl')\n",
    "        \n",
    "        with open(f'{path}/{self.model_type}_metrics.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "\n",
    "# Execução principal\n",
    "if __name__ == \"__main__\":\n",
    "    URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
    "    MODELOS = ['logistic', 'random_forest', 'xgboost', 'lightgbm']\n",
    "    METRICAS = ['roc_auc', 'f1', 'average_precision']\n",
    "    all_results = {}\n",
    "\n",
    "    for modelo in MODELOS:\n",
    "        for metrica in METRICAS:\n",
    "            print(f\"\\n==> Treinando {modelo} com métrica {metrica}\")\n",
    "            cr = CreditRiskModel(modelo, metrica)\n",
    "            res = cr.train_and_evaluate(URL)\n",
    "            cr.save_results(res)\n",
    "            all_results[f\"{modelo}_{metrica}\"] = res\n",
    "            print(f\"AUC: {res['metrics']['roc_auc']:.4f} | F1: {res['metrics']['f1']:.4f} | Tempo: {res['training_time']:.2f}s\")\n",
    "\n",
    "    with open('./models/all_results.json', 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
