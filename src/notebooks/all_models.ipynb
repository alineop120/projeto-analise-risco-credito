{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614bb362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from typing import Dict, Any, Tuple\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import (roc_auc_score, average_precision_score,\n",
    "                             f1_score, accuracy_score, precision_score,\n",
    "                             recall_score, make_scorer)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "from scipy.stats import loguniform, randint\n",
    "from joblib import Memory, dump\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "memory = Memory(location='./.cache', verbose=0)\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "class CreditRiskModel:\n",
    "    def __init__(self, model_type: str, metric: str = 'roc_auc') -> None:\n",
    "        self.model_type = model_type\n",
    "        self.metric = metric\n",
    "        self.best_model = None\n",
    "        self.metrics_history = []\n",
    "        self.scorer = self._get_scorer()\n",
    "        self._setup_model_config()\n",
    "\n",
    "    def _get_scorer(self):\n",
    "        scoring = {\n",
    "            'roc_auc': make_scorer(roc_auc_score, needs_proba=True),\n",
    "            'precision': make_scorer(precision_score),\n",
    "            'recall': make_scorer(recall_score),\n",
    "            'f1': make_scorer(f1_score),\n",
    "            'accuracy': make_scorer(accuracy_score),\n",
    "            'average_precision': make_scorer(average_precision_score, needs_proba=True)\n",
    "        }\n",
    "        return scoring.get(self.metric, scoring['roc_auc'])\n",
    "\n",
    "    def _setup_model_config(self) -> None:\n",
    "        self.model_config = {\n",
    "            'logistic': {\n",
    "                'model': LogisticRegression(max_iter=2000, solver='lbfgs', class_weight='balanced'),\n",
    "                'params': {\n",
    "                    'classifier__C': loguniform(1e-3, 1e2)\n",
    "                }\n",
    "            },\n",
    "            'random_forest': {\n",
    "                'model': RandomForestClassifier(class_weight='balanced', n_jobs=-1),\n",
    "                'params': {\n",
    "                    'classifier__n_estimators': randint(200, 500),\n",
    "                    'classifier__max_depth': randint(10, 50),\n",
    "                    'classifier__min_samples_split': randint(2, 20),\n",
    "                    'classifier__min_samples_leaf': randint(1, 10)\n",
    "                }\n",
    "            },\n",
    "            'xgboost': {\n",
    "                'model': XGBClassifier(use_label_encoder=False, eval_metric='logloss', n_jobs=-1),\n",
    "                'params': {\n",
    "                    'classifier__n_estimators': randint(150, 400),\n",
    "                    'classifier__learning_rate': loguniform(0.01, 0.2),\n",
    "                    'classifier__max_depth': randint(3, 10),\n",
    "                    'classifier__subsample': loguniform(0.5, 1.0),\n",
    "                    'classifier__colsample_bytree': loguniform(0.5, 1.0)\n",
    "                }\n",
    "            },\n",
    "            'lightgbm': {\n",
    "                'model': LGBMClassifier(class_weight='balanced', n_jobs=-1),\n",
    "                'params': {\n",
    "                    'classifier__n_estimators': randint(150, 400),\n",
    "                    'classifier__learning_rate': loguniform(0.01, 0.2),\n",
    "                    'classifier__max_depth': randint(3, 10),\n",
    "                    'classifier__num_leaves': randint(15, 100)\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def load_data(self, url: str) -> pd.DataFrame:\n",
    "        column_names = [\n",
    "            \"Status\", \"Duration\", \"CreditHistory\", \"Purpose\", \"CreditAmount\", \"Savings\",\n",
    "            \"Employment\", \"InstallmentRate\", \"PersonalStatusSex\", \"OtherDebtors\", \"ResidenceSince\",\n",
    "            \"Property\", \"Age\", \"OtherInstallmentPlans\", \"Housing\", \"ExistingCredits\",\n",
    "            \"Job\", \"NumPeopleLiable\", \"Telephone\", \"ForeignWorker\", \"Target\"\n",
    "        ]\n",
    "        df = pd.read_csv(url, sep=' ', header=None, names=column_names)\n",
    "        df['Target'] = df['Target'].map({1: 0, 2: 1})\n",
    "        return df\n",
    "\n",
    "    def prepare_data(self, df: pd.DataFrame) -> Tuple[Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series], ColumnTransformer, list]:\n",
    "        X = df.drop('Target', axis=1)\n",
    "        y = df['Target']\n",
    "        cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "        num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "        numeric_transformer = Pipeline([('scaler', StandardScaler())])\n",
    "        categorical_transformer = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "        preprocessor = ColumnTransformer(transformers=[\n",
    "            ('num', numeric_transformer, num_cols),\n",
    "            ('cat', categorical_transformer, cat_cols)\n",
    "        ])\n",
    "\n",
    "        return train_test_split(X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE), preprocessor, cat_cols\n",
    "\n",
    "    def build_pipeline(self, preprocessor: ColumnTransformer, cat_cols: list) -> ImbPipeline:\n",
    "        return ImbPipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('smote', SMOTENC(categorical_features=[i for i, col in enumerate(preprocessor.transformers[1][2])], random_state=RANDOM_STATE)),\n",
    "            ('classifier', clone(self.model_config[self.model_type]['model']))\n",
    "        ])\n",
    "\n",
    "    def optimize_model(self, pipeline: ImbPipeline, X_train: pd.DataFrame, y_train: pd.Series):\n",
    "        search = RandomizedSearchCV(\n",
    "            pipeline,\n",
    "            self.model_config[self.model_type]['params'],\n",
    "            n_iter=25,\n",
    "            cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=RANDOM_STATE),\n",
    "            scoring=self.scorer,\n",
    "            n_jobs=-1,\n",
    "            random_state=RANDOM_STATE,\n",
    "            verbose=0\n",
    "        )\n",
    "        search.fit(X_train, y_train)\n",
    "        return search.best_estimator_, search.best_params_\n",
    "\n",
    "    def evaluate_model(self, model, X_test, y_test) -> Dict[str, float | str]:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        return {\n",
    "            'roc_auc': roc_auc_score(y_test, y_proba),\n",
    "            'average_precision': average_precision_score(y_test, y_proba),\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1': f1_score(y_test, y_pred),\n",
    "            'optimization_metric': self.metric\n",
    "        }\n",
    "\n",
    "    def train_and_evaluate(self, url: str) -> Dict[str, Any]:\n",
    "        df = self.load_data(url)\n",
    "        (X_train, X_test, y_train, y_test), preprocessor, cat_cols = self.prepare_data(df)\n",
    "\n",
    "        start = time.time()\n",
    "        pipeline = self.build_pipeline(preprocessor, cat_cols)\n",
    "        best_model, best_params = self.optimize_model(pipeline, X_train, y_train)\n",
    "        train_time = time.time() - start\n",
    "\n",
    "        metrics = self.evaluate_model(best_model, X_test, y_test)\n",
    "        results = {\n",
    "            'model_type': self.model_type,\n",
    "            'best_params': best_params,\n",
    "            'metrics': metrics,\n",
    "            'training_time': train_time\n",
    "        }\n",
    "\n",
    "        self.best_model = best_model\n",
    "        self.metrics_history.append(results)\n",
    "        return results\n",
    "\n",
    "    def save_results(self, results: Dict[str, Any], path: str = '../models') -> None:\n",
    "        os.makedirs(path, exist_ok=True)\n",
    "        dump(self.best_model, f'{path}/{self.model_type}_model.pkl')\n",
    "        with open(f'{path}/{self.model_type}_metrics.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017a88d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Iniciando treino para logistic com métrica roc_auc...\n",
      "[INFO] Iniciando treino para logistic com métrica f1...\n",
      "[INFO] Iniciando treino para logistic com métrica average_precision...\n",
      "[INFO] Iniciando treino para random_forest com métrica roc_auc...\n",
      "[INFO] Iniciando treino para random_forest com métrica f1...\n",
      "[INFO] Iniciando treino para random_forest com métrica average_precision...\n",
      "[INFO] Iniciando treino para xgboost com métrica roc_auc...\n",
      "[INFO] Iniciando treino para xgboost com métrica f1...\n",
      "[INFO] Iniciando treino para xgboost com métrica average_precision...\n",
      "[INFO] Iniciando treino para lightgbm com métrica roc_auc...\n",
      "[INFO] Iniciando treino para lightgbm com métrica f1...\n",
      "[INFO] Iniciando treino para lightgbm com métrica average_precision...\n",
      "[INFO] Finalizado logistic - roc_auc | AUC: 0.8054 | F1: 0.6667 | Tempo: 1750.36s\n",
      "[INFO] Finalizado random_forest - f1 | AUC: 0.7874 | F1: 0.5902 | Tempo: 1766.69s\n",
      "[INFO] Finalizado random_forest - roc_auc | AUC: 0.7879 | F1: 0.5833 | Tempo: 1773.38s\n",
      "[INFO] Finalizado logistic - f1 | AUC: 0.8031 | F1: 0.6763 | Tempo: 1788.98s\n",
      "[INFO] Finalizado logistic - average_precision | AUC: 0.8054 | F1: 0.6667 | Tempo: 1789.72s\n",
      "[INFO] Finalizado random_forest - average_precision | AUC: 0.7904 | F1: 0.5763 | Tempo: 1791.81s\n",
      "[INFO] Finalizado xgboost - f1 | AUC: 0.8026 | F1: 0.6102 | Tempo: 1792.49s\n",
      "[INFO] Finalizado xgboost - roc_auc | AUC: 0.8015 | F1: 0.6111 | Tempo: 1798.00s\n",
      "[INFO] Finalizado xgboost - average_precision | AUC: 0.8015 | F1: 0.6111 | Tempo: 1798.11s\n",
      "[INFO] Finalizado lightgbm - f1 | AUC: 0.7845 | F1: 0.5926 | Tempo: 1819.91s\n",
      "[INFO] Finalizado lightgbm - average_precision | AUC: 0.7857 | F1: 0.5932 | Tempo: 1822.40s\n",
      "[INFO] Finalizado lightgbm - roc_auc | AUC: 0.7857 | F1: 0.5932 | Tempo: 1825.29s\n"
     ]
    }
   ],
   "source": [
    "URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\"\n",
    "MODELOS = ['logistic', 'random_forest', 'xgboost', 'lightgbm']\n",
    "METRICAS = ['roc_auc', 'f1', 'average_precision']\n",
    "all_results: Dict[str, Any] = {}\n",
    "\n",
    "def execute(modelo_metrica: Tuple[str, str]) -> Tuple[str, Dict[str, Any]]:\n",
    "    modelo, metrica = modelo_metrica\n",
    "    print(f\"[INFO] Iniciando treino para {modelo} com métrica {metrica}...\")\n",
    "    cr = CreditRiskModel(modelo, metrica)\n",
    "    res = cr.train_and_evaluate(URL)\n",
    "    cr.save_results(res)\n",
    "    print(f\"[INFO] Finalizado {modelo} - {metrica} | AUC: {res['metrics']['roc_auc']:.4f} | F1: {res['metrics']['f1']:.4f} | Tempo: {res['training_time']:.2f}s\")\n",
    "    return (f\"{modelo}_{metrica}\", res)\n",
    "\n",
    "combos = [(m, metrica) for m in MODELOS for metrica in METRICAS]\n",
    "with ThreadPoolExecutor() as executor:\n",
    "    for chave, resultado in executor.map(execute, combos):\n",
    "        all_results[chave] = resultado\n",
    "\n",
    "with open('../models/all_results.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
